{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization, Dense, Conv1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Deep Leaning class performs the following functions:\n",
    "#### 1. Given type of data i.e. images or dataframe, it generates dataset from flow of directory\n",
    "#### 2. Given a model and optimizer, it trains the neural net\n",
    "class NeuralNet():\n",
    "    def __init__(self, train_data_path, test_data_path, target_size=(0,0), is_image_data=True, epoch=None):\n",
    "        self.is_image_data = is_image_data\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "        if((self.is_image_data)&(epoch==None)): self.epoch = 30\n",
    "        elif((~self.is_image_data)&(epoch==None)): self.epoch = 300\n",
    "        else: self.epoch = epoch\n",
    "        self.target_size = target_size\n",
    "        \n",
    "    def dataset_generator(self, train_path, test_path, color_mode, class_mode, shuffle=True):\n",
    "        ## ImageDataGenerator is used to create dataset of images from directory mentioned in the function\n",
    "        traindata = ImageDataGenerator().flow_from_directory(directory=train_path, target_size=self.target_size, class_mode=class_mode,\n",
    "                                                             color_mode=color_mode, shuffle=shuffle)\n",
    "        testdata = ImageDataGenerator().flow_from_directory(directory=test_path, target_size=self.target_size, class_mode=class_mode,\n",
    "                                                             color_mode=color_mode, shuffle=shuffle)\n",
    "\n",
    "        return traindata, testdata\n",
    "    \n",
    "    def dataset_generator_cnn(self, train_path, test_path):\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "\n",
    "        ## Standardization\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data.drop(['label','name'],axis=1))\n",
    "        x_train = scaler.transform(train_data.drop(['label','name'],axis=1))\n",
    "        x_test = scaler.transform(test_data.drop(['label','name'],axis=1))\n",
    "\n",
    "        ## PCA\n",
    "        pca = PCA(.95)\n",
    "\n",
    "        train_pca = pca.fit_transform(x_train)\n",
    "        test_pca = pca.transform(x_test)\n",
    "\n",
    "        ## Labels Preparation\n",
    "        y_train = np.array(train_data.label)\n",
    "        y_test = np.array(test_data.label)\n",
    "        lb = LabelEncoder()\n",
    "        y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "        y_test = np_utils.to_categorical(lb.transform(y_test))\n",
    "        \n",
    "        ## Expanding Dimensions\n",
    "        x_train = np.expand_dims(train_pca, axis=2)\n",
    "        x_test = np.expand_dims(test_pca, axis=2)\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "        \n",
    "    def get_model(self):\n",
    "        ## This will generate a model summary.\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def fit(self, model, optimizer, loss, output_path):\n",
    "        ## This will compile the model and make it ready for training.\n",
    "        self.model = model\n",
    "        self.get_model()\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])\n",
    "        \n",
    "        ## Checkpoint is used to store the best model during training process on the basis of validation set accuracy\n",
    "        checkpoint = ModelCheckpoint(output_path, monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                     save_weights_only=False, mode='auto', period=1)\n",
    "        \n",
    "        ## Early stopping is used to avoid model overfitting. \n",
    "        ##If the model accuracy on validation set does not increase in 20 consecutive iterations the model will stop training further.\n",
    "        early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "             \n",
    "        ## Generate datasets\n",
    "        if self.is_image_data:\n",
    "            traindata, testdata = self.dataset_generator(self.train_data_path, self.test_data_path, color_mode='rgb', \n",
    "                                                         class_mode='categorical', shuffle=True)\n",
    "            self.hist = self.model.fit_generator(generator=traindata, validation_data=testdata, epochs=self.epoch,\n",
    "                                                 callbacks=[checkpoint,early])\n",
    "        else:\n",
    "            x_train, y_train, x_test, y_test = self.dataset_generator_cnn(self.train_data_path, self.test_data_path)\n",
    "            self.hist = self.model.fit(x_train, y_train, batch_size=16, epochs=self.epoch,\n",
    "                                       validation_data=(x_test, y_test), callbacks=[checkpoint, early])\n",
    "        \n",
    "        ## This will save model accuracy and loss and also check for early stopping and whether the model performs \n",
    "        ## better than the model in the previous iteration.\n",
    "\n",
    "        ## Generate Loss and Accuracy plots\n",
    "        self.accplot = self.get_plot([self.hist.history['acc'],self.hist.history['val_acc']],'a')\n",
    "        self.lossplot = self.get_plot([self.hist.history['loss'],self.hist.history['val_loss']],'l')\n",
    "    \n",
    "    def get_plot(self, metric, typeof):\n",
    "        if(typeof=='a'):\n",
    "            title = \"Model Accuracy\"\n",
    "            ylabel = \"Accuracy\"\n",
    "            xlabel = \"Epoch\"\n",
    "            legend = [\"Accuracy\",\"Validation Accuracy\"]\n",
    "        else:\n",
    "            title = \"Model Loss\"\n",
    "            ylabel = \"Loss\"\n",
    "            xlabel = \"Epoch\"\n",
    "            legend = [\"Loss\",\"Validation Loss\"]  \n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(metric[0])\n",
    "        plt.plot(metric[1])\n",
    "        plt.title(title)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.legend(legend)\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## For VGG 16 with Image Data\n",
    "input_shape = (350,350,3)\n",
    "model_input = Input(shape=input_shape, name='image_input')\n",
    "\n",
    "#Get back the convolutional part of a VGG 16 network\n",
    "model_vgg16 = VGG16(weights=None, include_top=False, input_tensor=model_input)\n",
    "\n",
    "## Adding my own dense layer\n",
    "model_output = model_vgg16(model_input)\n",
    "\n",
    "vggmodel = Flatten(name='flatten')(model_output)\n",
    "vggmodel = Dense(4096, activation='relu', name='fc1')(vggmodel)\n",
    "vggmodel = Dense(4096, activation='relu', name='fc2')(vggmodel)\n",
    "vggmodel = Dropout(0.5)(vggmodel)\n",
    "vggmodel = Dense(16, activation='softmax', name='predictions')(vggmodel)\n",
    "\n",
    "model = Model(inputs=model_input, outputs=vggmodel)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.SGD(lr = 0.001, momentum=0.9, decay=0.0)\n",
    "\n",
    "neuralnet = NeuralNet('data/spectrograms/train', 'data/spectrograms/test', (input_shape[0],input_shape[1]), True, 1)\n",
    "neuralnet.fit(model, optimizer, 'categorical_crossentropy', \"model/VGG16 with Mel Spectrograms.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For CNN with Dataframe of MFCC, MEL Scale, and Chroma\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(66,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr = 0.001, momentum=0.9, decay=0.0)\n",
    "\n",
    "neuralnet = NeuralNet(\"data/waves/train_wave_features.csv\", \"data/waves/test_wave_features.csv\", is_image_data=False, epoch=3)\n",
    "neuralnet.fit(model, optimizer, 'categorical_crossentropy', \"model/CNN with Feature Array.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
